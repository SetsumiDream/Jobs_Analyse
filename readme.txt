
Jobs_Analyse 开发日志

----------------------------------------------------------

开始时间
2019/12/26

----------------------------------------------------------

项目需求：
	1. 爬虫框架获取招聘网站
	   网站: 拉钩、智联、前程无忧、BOSS直聘
	   需求: 岗位信息，存入数据库，再从数据库导出，形成DF文件
	2. 数据分析
	   需求: 数据分析，机器学习
	3. web页面显示数据情况
	   需求: 页面输入信息爬取，生成分析

----------------------------------------------------------

项目概要设计和详细设计:
	1.待写

----------------------------------------------------------

编码(进度):
	2019/12/26
	 1.创建Git版本管理，创建基本环境 .jobenv
	   创建master、develop、jobs_spider分支
	 2.jobs_spider分支的编写

	2019/12/27
	 1.jobs_spider分支的编写

----------------------------------------------------------

测试报告:
	1.

----------------------------------------------------------

验收/发布上线/维护
	占位

----------------------------------------------------------
项目起始记录
----------------------------------------------------------

2019/12/26 详细记录

----------------------------------------------------------

Git项目版本管理

1. 创建虚拟环境
	virtualenv .webenv
	pip install django==1.11.18 mysqlclient redis

2. 创建django项目
	django-admin startproject 

3. git上创建仓库，项目进行git init
	git bash here
	git init

	vi .gitignore
	.git/
	.idea/
	.gitignore

	git add .
	git commit -m '创建了项目'
	git remote add origin git@github.com:SetsumiDream/Jobs_Analyse.git
	git push -u origin master

	git checkout -b develop  # 创建分支
	git push -u origin develop  # 上传分支

	# 设置权限
	Branches 
	add rule

	git checkout -b jobs_spider
	git push -u origin jobs_spider

	修改默认上传
	jobs_spider

jobs_spider分支的编写

scrapy在windows上安装有时会报错
解决办法:
  1.pip install pywin32
  2.https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
    下载对应版本Twisted
    pip install 路径文件

1. 安装 pip install scrapy

----------------------------------------------------------

2019/12/27 详细记录

----------------------------------------------------------



----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------
----------------------------------------------------------